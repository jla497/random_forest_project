@ARTICLE {Brieman01,
    author  = "Brieman, Leo",
    title   = "Random Forests",
    journal = "Machine Learning",
    year    = "2001",
    volume  = "45",
    number  = "1",
    pages   = "5-32"
}

@ARTICLE {Aslam07,
    author  = {Aslam, J.A. and Popa, R.A. and Rives,  R.L.},
    title   = {On Estimating the Size and Confidence of a Statistical Audit},
    journal = {Proc. Usenix/Accurate Electronic Voting Technology on Usenix/Accurate Electronic Voting Technology Workshop},
    year    = {2007},
    volume  = "",
    number  = "",
    pages   = "8"
}


@ARTICLE {Ho98,
    author  = {Ho, T.K.},
    title   = {The random subspace method for constructing decision forests},
    journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
    year    = {1998},
    volume  = "20",
    number  = "8",
    pages   = "832-844"
}

@ARTICLE {Ho93,
    author  = {Ho, T.K.},
    title   = {Recognition of Handwritten Digits by Combining Independent Learning Vector Quantizations},
    journal = {Proceedings of the Second International Conference on Document Analysis and Recognition},
    year    = {1993},
    volume  = "",
    number  = "",
    pages   = "818-821"
}

@ARTICLE {Ho95,
    author  = {Ho, T.K.},
    title   = {Random decision forests},
    journal = {Proceedings of the Third International Conference on Document Analysis and Recognition},
    year    = {1995},
    volume  = "",
    number  = "",
    pages   = "278-282"
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/Robin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}