@ARTICLE {Brieman01,
    author  = "Brieman, Leo",
    title   = "Random Forests",
    journal = "Machine Learning",
    year    = "2001",
    volume  = "45",
    number  = "1",
    pages   = "5-32"
}

@ARTICLE {Aslam07,
    author  = {Aslam, J.A. and Popa, R.A. and Rives,  R.L.},
    title   = {On Estimating the Size and Confidence of a Statistical Audit},
    journal = {Proc. Usenix/Accurate Electronic Voting Technology on Usenix/Accurate Electronic Voting Technology Workshop},
    year    = {2007},
    volume  = "",
    number  = "",
    pages   = "8"
}


@ARTICLE {Ho98,
    author  = {Ho, T.K.},
    title   = {The random subspace method for constructing decision forests},
    journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
    year    = {1998},
    volume  = "20",
    number  = "8",
    pages   = "832-844"
}

@ARTICLE {Ho93,
    author  = {Ho, T.K.},
    title   = {Recognition of Handwritten Digits by Combining Independent Learning Vector Quantizations},
    journal = {Proceedings of the Second International Conference on Document Analysis and Recognition},
    year    = {1993},
    volume  = "",
    number  = "",
    pages   = "818-821"
}

@ARTICLE {Ho95,
    author  = {Ho, T.K.},
    title   = {Random decision forests},
    journal = {Proceedings of the Third International Conference on Document Analysis and Recognition},
    year    = {1995},
    volume  = "",
    number  = "",
    pages   = "278-282"
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {https://www.cs.toronto.edu/{~}hinton/absps/JMLRdropout.pdf},
volume = {15},
year = {2014}
}

@article{Lock2014,
abstract = {Before any play of a National Football League (NFL) game, the probability that a given team will win depends on many situational variables (such as time remaining, yards to go for a first down, field position and current score) as well as the relative quality of the two teams as quantified by the Las Vegas point spread. We use a random forest method to combine pre-play variables to estimate Win Probability (WP) before any play of an NFL game. When a subset of NFL play-by-play data for the 12 seasons from 2001 to 2012 is used as a training dataset, our method provides WP estimates that resemble true win probability and accurately predict game outcomes, especially in the later stages of games. In addition to being intrinsically interesting in real time to observers of an NFL football game, our WP estimates can provide useful evaluations of plays and, in some cases, coaching decisions. ABSTRACT FROM AUTHOR},
author = {Lock, Dennis and Nettleton, Dan},
doi = {10.1515/jqas-2013-0100},
issn = {15590410},
journal = {Journal of Quantitative Analysis in Sports},
keywords = {NFL,random forest,win probability},
number = {2},
pages = {197--205},
title = {{Using random forests to estimate win probability before each play of an NFL game}},
url = {http://homepage.divms.uiowa.edu/{~}dzimmer/sports-statistics/nettletonandlock.pdf},
volume = {10},
year = {2014}
}

@article{Cutler2007,
author = {Cutler, D Richard and Edwards, Thomas C and Beard, Karen H and Cutler, Adele and Kyle, T and Gibson, Jacob and Lawler, Joshua J and Beard, H and Hess, T},
doi = {10.1890/07-0539.1},
isbn = {0012-9658},
issn = {0012-9658},
journal = {Ecology},
keywords = {partial dependence plots},
number = {11},
pages = {2783--2792},
pmid = {18051647},
title = {{Random Forests for Classification in Ecology}},
url = {http://digitalcommons.usu.edu/wild{\_}facpub},
volume = {88},
year = {2007}
}

@article{Bharathidason2014,
abstract = {Random forest can achieve high classification performance through a classification ensemble with a set of decision trees that grow using randomly selected subspaces of data. The performance of an ensemble learner is highly dependent on the accuracy of each component learner and the diversity among these components. In random forest, randomization would cause occurrence of bad trees and may include correlated trees. This leads to inappropriate and poor ensemble classification decision. In this paper an attempt has been made to improve the performance of the model by including only uncorrelated high performing trees in a random forest. Experimental results have shown that, the random forest can be further enhanced in terms of the classification accuracy.},
author = {Bharathidason, S and Venkataeswaran, Jothi C},
journal = {International Journal of Computer Applications},
keywords = {correlation,decision trees,strength,tree performance},
number = {13},
pages = {26--30},
title = {{Improving Classification Accuracy based on Random Forest Model with Uncorrelated High Performing Trees}},
volume = {101},
year = {2014}
}

@electronic{formann-roe_2012, 
title={Bias and Variance}, 
url={http://scott.fortmann-roe.com/docs/BiasVariance.html}, 
journal={Understanding the Bias-Variance Tradeoff}, 
author={Formann-Roe, Scott},
year={2012}, 
month={Jun}
}

@article{Harris2015,
abstract = {A recent method for mapping lithology which involves the Random Forest (RF) machine classification algorithm is evaluated. Random Forests, a supervised classifier, requires training data representative of each lithology to produce a predictive or classified map. We use two training strategies, one based on the location of lake sediment geochemical samples where the rock type is recorded from a legacy geology map at each sample station and the second strategy is based on lithology recorded from field stations derived from reconnaissance field mapping. We apply the classification to interpolated major and minor lake sediment geochemical data as well as airborne total field magnetic and gamma ray spectrometer data. Using this method we produce predictions of the lithology of a large section of the Hearne Archean - Paleoproterozoic tectonic domain, in northern Canada.The results indicate that meaningful predictive lithologic maps can be produced using RF classification for both training strategies. The best results were achieved when all data were used; however, the geochemical and gamma ray data were the strongest predictors of the various lithologies. The maps generated from this research can be used to compliment field mapping activities by focusing field work on areas where the predicted geology and legacy geology do not match and as first order geological maps in poorly mapped areas.},
author = {Harris, J. R. and Grunsky, E. C.},
doi = {10.1016/j.cageo.2015.03.013},
isbn = {00983004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {Classification,Geochemistry,Geological mapping,Geophysics,Random Forests},
pages = {9--25},
title = {{Predictive lithological mapping of Canada's North using Random Forest classification applied to geophysical and geochemical data}},
url = {https://ac-els-cdn-com.proxy.lib.sfu.ca/S0098300415000709/1-s2.0-S0098300415000709-main.pdf?{\_}tid=db1fa03a-d53e-11e7-9b20-00000aacb35d{\&}acdnat=1511985340{\_}14457261fb2002f25f3f9bfdb92ba8a3},
volume = {80},
year = {2015}
}

@article{Luo2017,
abstract = {Segmentation of the left atrium (LA) from cardiac magnetic resonance imaging (MRI) datasets is of great importance for image guided atrial fibrillation ablation, LA fibrosis quantification, and cardiac biophysical modelling. However, automated LA segmentation from cardiac MRI is challenging due to limited image resolution, considerable variability in anatomical structures across subjects, and dynamic motion of the heart. In this work, we propose a combined random forests (RFs) and active contour model (ACM) approach for fully automatic segmentation of the LA from cardiac volumetric MRI. Specifically, we employ the RFs within an autocontext scheme to effectively integrate contextual and appearance information from multisource images together for LA shape inferring. The inferred shape is then incorporated into a volume-scalable ACM for further improving the segmentation accuracy. We validated the proposed method on the cardiac volumetric MRI datasets from the STACOM 2013 and HVSMR 2016 databases and showed that it outperforms other latest automated LA segmentation methods. Validation metrics, average Dice coefficient (DC) and average surface-to-surface distance (S2S), were computed as and mm, versus those of 0.6222–0.878 and 1.34–8.72 mm, obtained by other methods, respectively.},
author = {Luo, Gongning and Wang, Kuanquan},
journal = {BioMed Research International},
keywords = {active contour model,cardiac mri,left atrium segmentation,random forest},
title = {{A combined random forest and active contour-model approach to fully automatic segmentation of the left atrium in volumetric MRI}},
volume = {2017},
year = {2017}
}

@article{Ghatasheh2014,
abstract = {See, stats, and : https : / / www . researchgate. net / publication / 268215588 Business for : A Article DOI : 10 . 14257 / ijast . 2014 . 72 . 02 CITATIONS 2 READS 364 1 : Some : Technology - Focused Trust Nazeeh University 51 SEE All . The . All - text and , letting . Abstract In the era of stringent and dynamic business environment , it is crucial for organizations to foresee their clients ' delinquency behavior . Such environment and behavior create unreliable base for strategic planning and risk management . Business Analytics combines the business expertise and computer intelligence to assist the decision makers by predicting an individual ' s credit status . This empirical research aims to evaluate the performance of different Machine Learning algorithms for credit risk prediction with more focus on Random Forest Trees . Several experiments inspired by observation and literature illustrate the potentials of computer - based model in classifying a number of bank history records . However , enhanced classification outcomes require tuning the randomness and tree growing parameters of the Random Forests algorithm . The model based on Random Forest Trees overperformed most of the other models . Moreover , such a model has various advantages to business experts as the ability to help in understanding the relations between the analyzed attributes .},
author = {Ghatasheh, Nazeeh},
doi = {10.14257/ijast.2014.72.02},
issn = {20054238},
journal = {International Journal of Advanced Science and Technology},
keywords = {Business Analytics,Decision Trees,Machine Learning,Random Decision Forest,Risk Prediction,Strategic Planning},
pages = {19--30},
title = {{Business Analytics using Random Forest Trees for Credit Risk Prediction: A Comparison Study}},
url = {https://www.researchgate.net/profile/Nazeeh{\_}Ghatasheh/publication/268215588{\_}Business{\_}Analytics{\_}using{\_}Random{\_}Forest{\_}Trees{\_}for{\_}Credit{\_}Risk{\_}Prediction{\_}A{\_}Comparison{\_}Study/links/546f292e0cf2d67fc0305645/Business-Analytics-using-Random-Forest-Trees-for-Credit-Risk-Prediction-A-Comparison-Study.pdf http://www.sersc.org/journals/IJAST/vol72/2.pdf},
volume = {72},
year = {2014}
}

