\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Implementation and Analysis of Random Forests}


\author{
Jae Lee\\
School of Computing Science\\
Simon Fraser University\\
Burnaby BC V5A 1S6 \\
%\texttt{email} \\
\And
Richard Mar \\
School of Computing Science\\
Simon Fraser University\\
Burnaby BC V5A 1S6 \\
%\texttt{email} \\
\AND
Robin White \\
School of Mechatronic Systems Engineering\\
Simon Fraser University\\
Surrey BC V3T 0A3 \\
%\texttt{email} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
TODO: Decide if we need this section.
\end{abstract}

\section{Introduction}

\subsection{Decision Trees}
TODO: Adjust titles as necessary and add content specifying other papers.

\subsection{Ensemble Learning}
\textbf{Bootstrap Aggregating (Bagging)}

Bootstrap aggregating or bagging is an ensemble learning technique that attempts to reduce variance of the model without increasing the bias by attempting to remove correlation between individual trees. Each tree is limited to evaluating only a random fractional sample of the actual dataset such that no two samples are the same. Bagging has been demonstrated empirically to improve model accuracy. [2]\\

From the actual dataset D of size n, k bootstrap samples, \[ D_1,D_2,..D_k  \] are randomly selected with repetition from D. 

Then the variance of the ensemble is the average of the sum of the individual trees' variances.

\[ var(L) = \frac{\sum_{i=1}^k var(L_i)}{k}\] where L is the ensemble of individual learners (trees).\\

Each datapoint in D has a probability of \[1-\left(\frac{1}{n}\right)^n = e^{-1} \approx 36.8 \% \] of not being selected in  a sample D\textsubscript{i} and therefore approximately 63.2\% probability of being in a training set D\textsubscript{i}.

\subsection{Random Forests}
TODO: Adjust titles as necessary and add content specifying other papers.

\section{Approach}
TODO: Figure out what is supposed to be here.

\section{Experiments}
TODO: Add content.

\subsection{Forest Size}
TODO: Add content.

\subsection{Tree Depth}
TODO: Add content.

\subsection{Splitting Criteria}

\textbf{Random Subspace Method}

Random forest uses a modified splitting algorithm that attempts to further reduce correlation between individual trees. For example, if few attributes are strong predictors of the target label, these attributes will be selected in many trees leading to high correlation and greater generalization error.  Generalization error of an ensemble converges to the following expression:

\[Generalization\ error \leq \frac{corr(1-s^2)}{s^2}\] where corr is the average correlation among the trees and s is the average performance of individual classifiers. Thus, reducing correlation among the individual trees will also lower the generalization error.

 Work by Ho has demonstrated that average tree agreement between trees is significantly lowered using the Random subspace method. [3] 
Estimating tree agreement between trees i and j as s\textsubscript{i,j}

\[ s_{i,j} = \frac{1}{n}\sum_{k=1}^{n}f(x_k)\]
\
\[ where \ f(x_k) = \begin{cases} 
      1 & if\ class\ decision_i(x_k) = class \ decision_j(x_k) \\
      0 & otherwise
   \end{cases}
\]

Ho's reult showed average of s\textsubscript{i,j} of random subspaces method was lower than that of bootstrapping and boosting methods alone. [3]

 Thus, limiting a tree's evaluation to only a subset of the actual feature set and randomizing this subset during the tree's splitting process helps to reduce correlation among each trees. The modified splitting algorithm will then split on a single feature with the best information gain ratio to reduce correlation among trees.

\subsubsection{Entropy}
TODO: Add content.

\subsubsection{Gini Index}
TODO: Add content.

\subsection{Custom Improvement?}



\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{TODO: Use this figure as a template for ours.}
\end{figure}

Table~\ref{sample-table}.

\begin{table}[t]
\caption{TODO: Use this table as a template for ours.}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf Machine Learning Package} &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Weka         &compare speed and accuracy at least \\
scikit-learn             &compare speed and accuracy at least \\
Tensorflow(?) Probably need one more, but not sure which one yet.             &compare speed and accuracy at least \\
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}
TODO: Add content.

\subsubsection*{Contributions}
See GitLab project here for specific commits:\\
\href{
    https://csil-git1.cs.surrey.sfu.ca/rkm3/mlclass-1777-randomforest
}{
    https://csil-git1.cs.surrey.sfu.ca/rkm3/mlclass-1777-randomforest
}

\subsubsection*{References}


\small{
[1] Leo Breiman. 2001. Random Forests. Machine Learning. 45 1, 5-32.

[2]  J.A. Aslam, R.A. Popa, R.L. Rivest. 2007. On Estimating the Size and Confidence of a Statistical Audit. Proc. Usenix/Accurate Electronic Voting Technology on Usenix/Accurate Electronic Voting Technology Workshop (EVT 07), pp. 8.

[3] T. K. Ho. 1998. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach. Intell., vol. 20, no. 8, pp. 832-844.

TODO: Add more citations and fix formatting of existing one if it's incorrect.
}

\end{document}
